{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import xlrd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "import datetime\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "import dateutil.relativedelta\n",
    "import calendar\n",
    "import re\n",
    "import pandas\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "前面都是def funciton，最后把所有function汇集起来\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "我手头的项目是要爬2010年到现在每个月/每一天的关于同居的微博。目前常见的微博爬虫有三个目标网站，一个是pc端口s.weibo.com，一个是智能手机端口m.weibo.cn，一个是非常老的手机端口weibo.cn。很多微博爬虫都采用智能手机端端口，因为可以爬到很多数据，只要不停往下滑应该就能爬完。但是我目前还没有发现智能手机端口如何搜索指定的日期或时间段，这就不得不用pc端口或者非常老的那个手机端口了。\n",
    "\n",
    "非常老的手机端口的好处在于不用管验证码，直接就可以登陆，省去了非常多的麻烦。而且不存在‘展开全文’的问题，pc端和智能手机端需要展开原文。而且每一个日期可以搜索到100页，pc端只能搜索到50页。虽然可能pc端每一页比手机端口要多，但是应该还是这个端口能爬到的更多一点。还有一个原因是可以直接勾选原创微博，省去了看上去爬了很多，但实际上爬完了还需要删除所有的转发的过程（虽然复制粘贴的还是要额外删除）。\n",
    "\n",
    "用selenium是因为微博会反爬虫，selenium可以模拟正常登陆的流程，而且一定要在每一次点击一个页面之后停顿几秒钟（sleep），不然很容易被封号甚至封IP。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spider_range(YEAR, target_month):\n",
    "    date = datetime(YEAR, target_month, 1)\n",
    "    for i in range(31):\n",
    "        date_reit.append(date.strftime(\"%Y\")+date.strftime(\"%m\")+date.strftime(\"%d\"))\n",
    "        if date == datetime(YEAR, target_month, 1) + dateutil.relativedelta.relativedelta(months=1)+ dateutil.relativedelta.relativedelta(days=-1):\n",
    "            break\n",
    "        else:\n",
    "            date = date + dateutil.relativedelta.relativedelta(days=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def login():   \n",
    "    #登陆 Login\n",
    "    driver.get('https://weibo.cn/pub/?vt=' )\n",
    "   \n",
    "    elem = driver.find_element_by_xpath(\"//*[@class='ut']/a[1]\").click();\n",
    "    time.sleep(1)\n",
    "    \n",
    "    print(\"开始自动登陆Loading\")\n",
    "    elem = driver.find_element_by_xpath(\"//*[@id='loginName']\");\n",
    "    elem.send_keys(username)\n",
    "    elem = driver.find_element_by_xpath(\"//*[@id='loginPassword']\");\n",
    "    elem.send_keys(password)\n",
    "    elem = driver.find_element_by_xpath(\"//*[@id='loginAction']\");\n",
    "    elem.send_keys(Keys.ENTER)  \n",
    "    time.sleep(2)\n",
    "    print('登陆完毕 Login Finish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#每爬完一天清理一次临时数据\n",
    "#clear temp data after each day\n",
    "def clean_temp_data():\n",
    "    usernames.clear()\n",
    "    user_links.clear()\n",
    "    posts.clear()\n",
    "    likes.clear()\n",
    "    reblogs.clear()\n",
    "    comments.clear()\n",
    "    comment_links.clear()\n",
    "    post_date.clear()\n",
    "    post_time.clear()\n",
    "    post_date_datetime.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#每天保存\n",
    "#save each day\n",
    "def save_data():\n",
    "    for post in posts:\n",
    "        data['Posts'].append(post)\n",
    "    for username in usernames:\n",
    "        data['User Names'].append(username)\n",
    "    for user_link in user_links:\n",
    "        data['User Links'].append(user_link)\n",
    "    for like in likes:\n",
    "        data['Likes'].append(like)\n",
    "    for reblog in reblogs:\n",
    "        data['Reblogs'].append(reblog)\n",
    "    for comment in comments:\n",
    "        data['Comments'].append(comment)\n",
    "    for comment_link in comment_links:\n",
    "        data['Comment Links'].append(comment_link)\n",
    "    for post_d in post_date:\n",
    "        data['Post Date'].append(post_d)\n",
    "    for post_t in post_time:\n",
    "        data['Post Time'].append(post_t)\n",
    "    for post_date_d in post_date_datetime:\n",
    "        data['Date in Datetime Format'].append(post_date_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#翻页 Next Page\n",
    "def click_next_page():\n",
    "    NextPage = driver.find_element_by_xpath(\"//a[contains(text(),'下页')]\")\n",
    "    NextPage.click()\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#开始爬一页微博。注意elements是可以抓取多个，element只能抓一个\n",
    "#Scraping one page. Notice elements not element. Elements can find multiple element\n",
    "\n",
    "def scrape_one_page():\n",
    "    #找到发微博的用户名和连接\n",
    "    #Get username and href link\n",
    "    elem = driver.find_elements_by_xpath(\"//*[@class='nk']\")\n",
    "    for post in elem:\n",
    "        usernames.append(post.text)\n",
    "        user_links.append(post.get_attribute('href'))\n",
    "\n",
    "    #找到微博内容\n",
    "    #Get post content\n",
    "    elem = driver.find_elements_by_xpath(\"//*[@class='ctt']\")\n",
    "    for post in elem:\n",
    "        posts.append(post.text)\n",
    "\n",
    "    #点赞\n",
    "    #likes\n",
    "    likes_temp = []\n",
    "    elem = driver.find_elements_by_xpath(\"//a[contains(text(),'赞[')]\")\n",
    "    for like in elem:\n",
    "        likes_temp.append(like.text)\n",
    "    for like in likes_temp:\n",
    "        like = re.sub(r'\\D', '', like) #去除掉‘赞’这个字只保留数字Delete 'likes' text\n",
    "        likes.append(like)\n",
    "    \n",
    "    #转发评论\n",
    "    #reblogs\n",
    "    reblog_temp = []\n",
    "    elem = driver.find_elements_by_xpath(\"//a[contains(text(),'转发[')]\")\n",
    "    for reblog in elem:\n",
    "        reblog_temp.append(reblog.text)\n",
    "    for reblog in reblog_temp:\n",
    "        reblog = re.sub(r'\\D', '', reblog) #保留数字keep the number\n",
    "        reblogs.append(reblog)  \n",
    "\n",
    "    #评论和评论的链接comment and links to comment section\n",
    "    comment_temp = []\n",
    "    elem = driver.find_elements_by_xpath(\"//a[contains(text(),'评论[')]\")\n",
    "    for comment in elem:\n",
    "        comment_temp.append(comment.text)\n",
    "        comment_links.append(comment.get_attribute('href'))#评论的链接links to comment\n",
    "    for comment in comment_temp:\n",
    "        comment = re.sub(r'\\D', '', comment) #保留数字keep the number\n",
    "        comments.append(comment)  \n",
    "\n",
    "    #发布日期和时间\n",
    "    elem = driver.find_elements_by_xpath(\"//*[@class='ct']\")\n",
    "    for time in elem:\n",
    "        time_txt = time.text.split(' ')\n",
    "        post_date.append(time_txt[0])\n",
    "        post_time.append(time_txt[1])\n",
    "        post_date_datetime.append(datetime(int(time_txt[0][0:4]),\n",
    "                                           int(time_txt[0][5:7]),\n",
    "                                           int(time_txt[0][8:10])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "#爬一天内的每一页\n",
    "#Scrape every page in a day\n",
    "def scrape_everything_in_a_day(target_page_per_day):    \n",
    "    \n",
    "    current_page = []\n",
    "    max_page = []\n",
    "    \n",
    "    #建立两个检测的目标，一个是看有没有内容，一个是看有没有页码。\n",
    "    #没有内容说明刷新太快被限制访问了，要返回等几秒再重新搜索\n",
    "    #有内容没有页码说明这一天只有一页，爬下来就结束了\n",
    "    #有内容有页码就循环一下每一页都爬下来\n",
    "    #Set up 2 variables. First for checking content, second for page\n",
    "    #If there is no content, we are refreshing too fast and restrained for access, go back and wait for a couple of seconds\n",
    "    #If content exists, but page number doesn't. This day only has one page\n",
    "    #If content exists, and page number also exists, reiterate through days\n",
    "    \n",
    "    try_for_content = len(driver.find_elements_by_xpath(\"//*[@class='nk']\"))\n",
    "    try_for_page = len(driver.find_elements_by_xpath(\"//*[@class='pa']\"))\n",
    "    #返回的list如果是空的说明没有\n",
    "    #If list is empty it means no content/page\n",
    "            \n",
    "    if try_for_content != 0 and try_for_page == 0:\n",
    "        #只有一页only one page\n",
    "        scrape_one_page()\n",
    "        save_data()\n",
    "        \n",
    "    elif try_for_content != 0 and try_for_page != 0:\n",
    "        elem = driver.find_element_by_xpath(\"//*[@class='pa']\").text.split('/')\n",
    "        max_page = elem[1]\n",
    "        max_page = re.sub(r'\\D','',max_page)\n",
    " \n",
    "        #如果超过最大页码，目标页码变为最大页码\n",
    "        #If target page exceeds max page, set it to max\n",
    "        if int(max_page) <int(target_page_per_day):\n",
    "            target_page = max_page\n",
    "        else:\n",
    "            target_page = target_page_per_day\n",
    "            \n",
    "        #开始循环reit\n",
    "        for page in range(int(target_page) - 1):\n",
    "            try_for_content = len(driver.find_elements_by_xpath(\"//*[@class='nk']\"))\n",
    "            try_for_page = len(driver.find_elements_by_xpath(\"//*[@class='pa']\"))\n",
    "            \n",
    "            if try_for_content != 0:\n",
    "                scrape_one_page()\n",
    "                \n",
    "            #如果被限制返回等20秒\n",
    "            #If restrained go back and wait for 20sec\n",
    "            else:\n",
    "                driver.back()\n",
    "                time.sleep(20)\n",
    "                \n",
    "                \n",
    "                elem = driver.find_element_by_xpath(\"//*[@class='pa']\").text.split('/')\n",
    "                current_page = elem[0]\n",
    "                current_page = re.sub(r'\\D','',current_page)\n",
    "\n",
    "                elem = driver.find_element_by_xpath(\"//*[@name='page']\")\n",
    "                jump_one_page = int(current_page) + 1\n",
    "                elem.clear()\n",
    "                elem.send_keys(jump_one_page)\n",
    "                driver.find_element_by_xpath(\"//*[@value='跳页']\").click()\n",
    "                \n",
    "                try_for_content = len(driver.find_elements_by_xpath(\"//*[@class='nk']\"))\n",
    "                try_for_page = len(driver.find_elements_by_xpath(\"//*[@class='pa']\"))\n",
    "                \n",
    "                #如果等几秒刷新出来了就爬\n",
    "                if try_for_content != 0:\n",
    "                    scrape_one_page()\n",
    "                #如果还是没有内容说明这一页就是没有内容，我们跳过\n",
    "                #If still no content, there's no content on this page, skip\n",
    "                else:\n",
    "                    driver.back()\n",
    "                    time.sleep(5)\n",
    "                    \n",
    "                    elem = driver.find_element_by_xpath(\"//*[@class='pa']\").text.split('/')\n",
    "                    current_page = elem[0]\n",
    "                    max_page = elem[1]\n",
    "                    current_page = re.sub(r'\\D','',current_page)\n",
    "                    max_page = re.sub(r'\\D','',max_page)\n",
    "                    \n",
    "                    #判断是否到倒数第二页,倒数第二页跳不了直接结束\n",
    "                    #if second last page. If true cant' skip, just break\n",
    "                    if int(current_page) == int(target_page)-1:\n",
    "                        break\n",
    "                    #除此之外跳过这一页otherwise skip this page\n",
    "                    else:\n",
    "                        elem = driver.find_element_by_xpath(\"//*[@name='page']\")\n",
    "                        jump_one_page = int(current_page) + 2\n",
    "                        elem.clear()\n",
    "                        elem.send_keys(jump_one_page)\n",
    "                        driver.find_element_by_xpath(\"//*[@value='跳页']\").click()\n",
    "                        try:\n",
    "                            driver.find_element_by_xpath(\"//*[@class='pa']\")\n",
    "                            scrape_one_page()\n",
    "                        #如果还不行的话我们再返回再等一等\n",
    "                        except NoSuchElementException:\n",
    "                            driver.back()\n",
    "                            time.sleep(30)\n",
    "                            elem = driver.find_element_by_xpath(\"//*[@name='page']\")\n",
    "                            elem.clear()\n",
    "                            elem.send_keys(jump_one_page)\n",
    "                            driver.find_element_by_xpath(\"//*[@value='跳页']\").click()\n",
    "                            \n",
    "                            try:\n",
    "                                driver.find_element_by_xpath(\"//*[@class='pa']\")\n",
    "                                scrape_one_page()    \n",
    "                            except NoSuchElementException:\n",
    "                                break\n",
    "\n",
    "            elem = driver.find_element_by_xpath(\"//*[@class='pa']\").text.split('/')\n",
    "            current_page = elem[0]\n",
    "            current_page = re.sub(r'\\D','',current_page)\n",
    "            if int(current_page) == int(target_page):\n",
    "                break\n",
    "            else:\n",
    "                click_next_page()\n",
    "        save_data()\n",
    "    \n",
    "    else:\n",
    "        driver.back()\n",
    "        driver.back()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#检测搜索完后第一页有没有内容有没有被限流\n",
    "#test for the first page after reit_search\n",
    "\n",
    "def test_first_page_in_a_day():\n",
    "    try_for_content = len(driver.find_elements_by_xpath(\"//*[@class='nk']\"))\n",
    "    timer = 0\n",
    "    time.sleep(1)\n",
    "    while try_for_content == 0:\n",
    "        timer = timer + 1\n",
    "        driver.back()\n",
    "        time.sleep(20) \n",
    "        driver.find_element_by_xpath(\"//*[@type='submit']\").click()\n",
    "        \n",
    "        try_for_content = len(driver.find_elements_by_xpath(\"//*[@class='nk']\"))\n",
    "        if try_for_content != 0:\n",
    "            break\n",
    "        elif timer == 2:\n",
    "            break\n",
    "            #if nothing on that date, break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来是搜索前后和按照日期循环\n",
    "\n",
    "Below are before and after search and reiterate through dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_search_page_first_time():\n",
    "    #点开搜索页面 Open search tap\n",
    "    driver.find_element_by_xpath(\"//a[contains(text(),'搜索')]\").click()\n",
    "    driver.find_element_by_xpath(\"//a[contains(text(),'高级搜索')]\").click()\n",
    "    time.sleep(1)\n",
    "    \n",
    "def keyword_input(key):\n",
    "    #输入关键词 Input Keywords\n",
    "    #我这里是\"同居\" In this case 'cohabitation'\n",
    "    elem = driver.find_element_by_xpath(\"//*[@type='text']\")\n",
    "    elem.send_keys(key)\n",
    "    time.sleep(1)\n",
    "    \n",
    "def date_input(day):\n",
    "    #开始年月日 Start data e.g, 20200101\n",
    "    elem = driver.find_element_by_xpath(\"//*[@name='starttime']\")\n",
    "    elem.clear()\n",
    "    elem.send_keys(day)\n",
    "    time.sleep(3)\n",
    "\n",
    "    #结束年月日一样 End data same\n",
    "    elem = driver.find_element_by_xpath(\"//*[@name='endtime']\")\n",
    "    elem.clear()\n",
    "    elem.send_keys(day)\n",
    "    time.sleep(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_first_time(key):\n",
    "    #上面def的两个function\n",
    "    open_search_page_first_time()\n",
    "    keyword_input(key)\n",
    "    \n",
    "    #勾选原创 Click original posts\n",
    "    driver.find_element_by_xpath(\"//*[@name='hasori']\").click()\n",
    "    \n",
    "    date_input(date_reit[0])\n",
    "    \n",
    "    #搜索 Search\n",
    "    driver.find_element_by_xpath(\"//*[@type='submit']\").click()\n",
    "    time.sleep(7)\n",
    "    \n",
    "def search_reiterate(day):\n",
    "    #点开‘更多’ Open'more'\n",
    "    #因为之前输入过关键词和勾选原创，这里可以省略\n",
    "    #Because we have input keywords and click original, we can skip here\n",
    "    elem = driver.find_elements_by_xpath(\"//a[contains(text(),'更多')]\")\n",
    "    \n",
    "    if elem != 0:\n",
    "        driver.find_element_by_xpath(\"//a[contains(text(),'更多')]\").click()\n",
    "        time.sleep(5)\n",
    "        try:\n",
    "            date_input(day)\n",
    "            #搜索 Search\n",
    "            driver.find_element_by_xpath(\"//*[@type='submit']\").click()\n",
    "            time.sleep(7)\n",
    "\n",
    "        except NoSuchElementException:\n",
    "            driver.back()\n",
    "            time.sleep(15)\n",
    "\n",
    "            driver.find_element_by_xpath(\"//a[contains(text(),'更多')]\").click()\n",
    "            time.sleep(7)\n",
    "\n",
    "            date_input(day)\n",
    "\n",
    "            #搜索 Search\n",
    "            driver.find_element_by_xpath(\"//*[@type='submit']\").click()\n",
    "            time.sleep(7)\n",
    "    else:\n",
    "        driver.back()\n",
    "        time.sleep(10)\n",
    "        driver.find_element_by_xpath(\"//a[contains(text(),'更多')]\").click()\n",
    "        time.sleep(3)\n",
    "        try:\n",
    "            date_input(day)\n",
    "            #搜索 Search\n",
    "            driver.find_element_by_xpath(\"//*[@type='submit']\").click()\n",
    "            time.sleep(7)\n",
    "\n",
    "        except NoSuchElementException:\n",
    "            driver.back()\n",
    "            time.sleep(15)\n",
    "\n",
    "            driver.find_element_by_xpath(\"//a[contains(text(),'更多')]\").click()\n",
    "            time.sleep(3)\n",
    "\n",
    "            date_input(day)\n",
    "\n",
    "            #搜索 Search\n",
    "            driver.find_element_by_xpath(\"//*[@type='submit']\").click()\n",
    "            time.sleep(7)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def login\n",
    "def search_first_time(keyword)\n",
    "def search_reiterate(day)\n",
    "def scrape_everything_in_a_day(target_page_per_day)\n",
    "def clean_temp_data()\n",
    "\n",
    "\n",
    "这些是目前定义过的比较重要的，接下来把他们拼起来就行\n",
    "These are important defs so far. We just need to put them together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_by_date(keyword, target_year, \n",
    "                   target_month, target_page_per_day):\n",
    "    \n",
    "    #创建指定年份的日期列表\n",
    "    #Create a list of dyas in target year\n",
    "    spider_range(target_year, target_month)\n",
    "    \n",
    "    #登陆\n",
    "    #login\n",
    "    login()\n",
    "    \n",
    "    #开始搜索指定关键词的第一天的原创微博\n",
    "    #search for keywords, click original, on the first day in the list\n",
    "    search_first_time(keyword)\n",
    "    \n",
    "    #爬+保存，清除临时数据列表\n",
    "    scrape_everything_in_a_day(target_page_per_day)\n",
    "    clean_temp_data()\n",
    "    print('Finish ' + date_reit[0])\n",
    "    \n",
    "    #从第二天开始循环，所以去掉列表里第一天\n",
    "    #Reit from second day, so delete first day in the list\n",
    "    reit_from_2nd_day = []\n",
    "    reit_from_2nd_day = date_reit[1:]\n",
    "    for day in reit_from_2nd_day:\n",
    "        search_reiterate(day)\n",
    "        test_first_page_in_a_day()\n",
    "        scrape_everything_in_a_day(target_page_per_day)\n",
    "        print('Finish ' + day)\n",
    "        clean_temp_data()\n",
    "    \n",
    "    print('Finish scraping in ' + str(target_year) + ' ' + str(target_month))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始自动登陆Loading\n",
      "登陆完毕 Login Finish\n",
      "Finish 20191201\n",
      "Finish 20191202\n",
      "Finish 20191203\n",
      "Finish 20191204\n",
      "Finish 20191205\n",
      "Finish 20191206\n",
      "Finish 20191207\n",
      "Finish 20191208\n",
      "Finish 20191209\n",
      "Finish 20191210\n",
      "Finish 20191211\n",
      "Finish 20191212\n",
      "Finish 20191213\n",
      "Finish 20191214\n",
      "Finish 20191215\n",
      "Finish 20191216\n",
      "Finish 20191217\n",
      "Finish 20191218\n",
      "Finish 20191219\n",
      "Finish 20191220\n",
      "Finish 20191221\n",
      "Finish 20191222\n",
      "Finish 20191223\n",
      "Finish 20191224\n",
      "Finish 20191225\n",
      "Finish 20191226\n",
      "Finish 20191227\n",
      "Finish 20191228\n",
      "Finish 20191229\n",
      "Finish 20191230\n",
      "Finish 20191231\n",
      "Finish scraping in 2019 12\n"
     ]
    }
   ],
   "source": [
    "#创建临时数据列表\n",
    "#Creat temp data\n",
    "date_reit = []\n",
    "usernames = []\n",
    "user_links = []\n",
    "posts = []\n",
    "likes = []\n",
    "reblogs = []\n",
    "comments = []\n",
    "comment_links = []\n",
    "post_date = []   \n",
    "post_time = []\n",
    "post_date_datetime = []\n",
    "data = {'Posts':[],\n",
    "    'User Names':[],\n",
    "    'User Links':[],\n",
    "    'Likes':[],\n",
    "    'Reblogs':[],\n",
    "    'Comments':[],\n",
    "    'Comment Links':[],\n",
    "    'Post Date':[],\n",
    "    'Post Time':[],\n",
    "    'Date in Datetime Format':[]}\n",
    "\n",
    "\n",
    "#正式跑起来\n",
    "#running\n",
    "if __name__ == '__main__':\n",
    "    username = \"17064888409\" #用户名username\n",
    "    password = \"rw297421\" #密码password        \n",
    "    keyword = ['同居']\n",
    "    target_year = 2019\n",
    "    target_month = 12\n",
    "    target_page_per_day = 3\n",
    "    \n",
    "    driver = webdriver.Chrome()#可以换成firefox.Can use firefox\n",
    "    for word in keyword:\n",
    "        scrape_by_date(word, target_year, \n",
    "                       target_month, target_page_per_day)\n",
    "#页数可以设置成max_page\n",
    "\n",
    "#有时候需要运行几次才能ok，第一次卡在登陆，第二次卡在前两天\n",
    "#sometimes it takes multiple tries...first time stuck at login, second time at first two days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "484\n",
      "484\n",
      "484\n",
      "484\n",
      "484\n",
      "484\n",
      "484\n",
      "484\n",
      "484\n",
      "484\n"
     ]
    }
   ],
   "source": [
    "#检查一下是不是数量相等，相等才能建起来datafram\n",
    "#check if numbers are equal so that we can turn this into dataframe\n",
    "print(len(data['User Names']))\n",
    "print(len(data['User Links']))\n",
    "print(len(data['Posts']))\n",
    "print(len(data['Likes']))\n",
    "print(len(data['Reblogs']))\n",
    "print(len(data['Comments']))\n",
    "print(len(data['Comment Links']))\n",
    "print(len(data['Post Date']))\n",
    "print(len(data['Post Time']))\n",
    "print(len(data['Date in Datetime Format']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_2012_1 = data\n",
    "df_2012_1 = pd.DataFrame(data_2012_1)\n",
    "df_2012_1.to_csv(r'data/2019/df_2019_1.csv', index = False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_2012_2 = data\n",
    "df_2012_2 = pd.DataFrame(data_2012_2)\n",
    "df_2012_2.to_csv(r'data/2019/df_2019_2.csv', index = False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_2012_3 = data\n",
    "df_2012_3 = pd.DataFrame(data_2012_3)\n",
    "df_2012_3.to_csv(r'data/2019/df_2019_3.csv', index = False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_2012_4 = data\n",
    "df_2012_4 = pd.DataFrame(data_2012_4)\n",
    "df_2012_4.to_csv(r'data/2019/df_2019_4.csv', index = False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_2012_5 = data\n",
    "df_2012_5 = pd.DataFrame(data_2012_5)\n",
    "df_2012_5.to_csv(r'data/2019/df_2019_5.csv', index = False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_2012_6 = data\n",
    "df_2012_6 = pd.DataFrame(data_2012_6)\n",
    "df_2012_6.to_csv(r'data/2019/df_2019_6.csv', index = False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_2012_7 = data\n",
    "df_2012_7 = pd.DataFrame(data_2012_7)\n",
    "df_2012_7.to_csv(r'data/2019/df_2019_7.csv', index = False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_2012_8 = data\n",
    "df_2012_8 = pd.DataFrame(data_2012_8)\n",
    "df_2012_8.to_csv(r'data/2019/df_2019_8.csv', index = False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_2010_9 = data\n",
    "df_2010_9 = pd.DataFrame(data_2010_9)\n",
    "df_2010_9.to_csv(r'data/2019/df_2019_9.csv', index = False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_2010_10 = data\n",
    "df_2010_10 = pd.DataFrame(data_2010_10)\n",
    "df_2010_10.to_csv(r'data/2019/df_2019_10.csv', index = False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_2010_11 = data\n",
    "df_2010_11 = pd.DataFrame(data_2010_11)\n",
    "df_2010_11.to_csv(r'data/2019/df_2019_11.csv', index = False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_2012_12 = data\n",
    "df_2012_12 = pd.DataFrame(data_2012_12)\n",
    "df_2012_12.to_csv(r'data/2019/df_2019_12.csv', index = False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2010_1 = pd.DataFrame(data_2010_1)\n",
    "df_2010_2 = pd.DataFrame(data_2010_2)\n",
    "df_2010_3 = pd.DataFrame(data_2010_3)\n",
    "df_2010_4 = pd.DataFrame(data_2010_4)\n",
    "df_2010_5 = pd.DataFrame(data_2010_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2010_1.to_csv(r'df_2010_1.csv', index = False, header=True)\n",
    "df_2010_2.to_csv(r'df_2010_2.csv', index = False, header=True)\n",
    "df_2010_3.to_csv(r'df_2010_3.csv', index = False, header=True)\n",
    "df_2010_4.to_csv(r'df_2010_4.csv', index = False, header=True)\n",
    "df_2010_5.to_csv(r'df_2010_5.csv', index = False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abd = pandas.read_csv('df_2010_1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2010_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = [df_2010_1, df_2010_2, df_2010_3,\n",
    "         df_2010_4, df_2010_5,]\n",
    "\n",
    "temp_save_2010_1_5 = pd.concat(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_save_2010_1_5['Posts']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2010_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_2010_5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_2010 = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_2010 = mergeDict(data, data_2010)\n",
    "print(len(data_2010['User Names']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_page = 3\n",
    "for page in range(int(target_page) - 1):\n",
    "    print(page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_2010['Posts']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(data_2010['User Names']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in data_2010.items():\n",
    "    velue = value[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_2010.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_2010['Posts']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Likes'].index('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Posts'][1409:1411]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrape_everything_in_a_day(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#指定年份和季度\n",
    "#set years and quarter\n",
    "\n",
    "def spider_range(YEAR, target_month):\n",
    "    YEAR = int(YEAR)\n",
    "    date = datetime(YEAR,1,1)\n",
    "    date_in_target_month = datetime(YEAR, target_month, 1)\n",
    "    days_in_year = 365 + (1*calendar.isleap(YEAR))\n",
    "    \n",
    "    for i in range(days_in_year):\n",
    "        days_normal.append(date.strftime(\"%Y\")+date.strftime(\"%m\")+date.strftime(\"%d\"))\n",
    "        date = date + dateutil.relativedelta.relativedelta(days=1) \n",
    "    \n",
    "    for day in days_normal[days_normal.index(str(YEAR) + '0401'):\n",
    "                     days_normal.index(str(YEAR) + '0701')]:\n",
    "            reit_dates.append(day)\n",
    "    \n",
    "    \n",
    "    \n",
    "    if quarter == 1:\n",
    "        for day in days_normal[:days_normal.index(str(YEAR) + '0401')]:\n",
    "            reit_dates.append(day)\n",
    "    \n",
    "    elif quarter == 2:\n",
    "        for day in days_normal[days_normal.index(str(YEAR) + '0401'):\n",
    "                     days_normal.index(str(YEAR) + '0701')]:\n",
    "            reit_dates.append(day)\n",
    "    \n",
    "    elif quarter == 3:\n",
    "        for day in days_normal[days_normal.index(str(YEAR) + '0701'):\n",
    "                     days_normal.index(str(YEAR) + '1001')]:\n",
    "            reit_dates.append(day)\n",
    "\n",
    "    elif quarter ==4:\n",
    "        for day in days_normal[days_normal.index(str(YEAR) + '1001'):]:\n",
    "            reit_dates.append(day)  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
