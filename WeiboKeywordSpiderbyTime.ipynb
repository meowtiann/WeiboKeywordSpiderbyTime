{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import xlrd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import datetime\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "import dateutil.relativedelta\n",
    "import calendar\n",
    "import re\n",
    "import pandas\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我手头的项目是要爬2010年到现在每个月/每一天的关于同居的微博。目前常见的微博爬虫有三个目标网站，一个是pc端口s.weibo.com，一个是智能手机端口m.weibo.cn，一个是非常老的手机端口weibo.cn。很多微博爬虫都采用智能手机端端口，因为可以爬到很多数据，只要不停往下滑应该就能爬完。但是我目前还没有发现智能手机端口如何搜索指定的日期或时间段，这就不得不用pc端口或者非常老的那个手机端口了。\n",
    "\n",
    "非常老的手机端口的好处在于不用管验证码，直接就可以登陆，省去了非常多的麻烦。而且不存在‘展开全文’的问题，pc端和智能手机端需要展开原文。而且每一个日期可以搜索到100页，pc端只能搜索到50页。虽然可能pc端每一页比手机端口要多，但是应该还是这个端口能爬到的更多一点。还有一个原因是可以直接勾选原创微博，省去了看上去爬了很多，但实际上爬完了还需要删除所有的转发的过程（虽然复制粘贴的还是要额外删除）。\n",
    "\n",
    "用selenium是因为微博会反爬虫，selenium可以模拟正常登陆的流程，而且一定要在每一次点击一个页面之后停顿几秒钟（sleep），不然很容易被封号甚至封IP。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "days_normal = []\n",
    "days_datetime = []\n",
    "usernames = []\n",
    "user_links = []\n",
    "posts = []\n",
    "likes = []\n",
    "reblogs = []\n",
    "comments = []\n",
    "comment_links = []\n",
    "post_date = []\n",
    "post_time = []\n",
    "data = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#我本来想按照月来爬的，但是输入指定月份例如20100101-20100131之后\n",
    "#爬到的几乎都是1月31号的微博，这样的样本不是很好\n",
    "#所以还是每天都爬2、3页当做样本，过几个星期我再每天都爬100页试试\n",
    "\n",
    "def dates_in_year(YEAR):\n",
    "    YEAR = int(YEAR)\n",
    "    MONTH = 1\n",
    "    DAY = 1\n",
    "    date = datetime(YEAR,MONTH,DAY)\n",
    "    \n",
    "    days_in_year = 365 + (1*calendar.isleap(YEAR))\n",
    "    for i in range(days_in_year):\n",
    "        days_normal.append(date.strftime(\"%Y\")+date.strftime(\"%m\")+date.strftime(\"%d\"))\n",
    "        days_datetime.append(date) \n",
    "        date = date + dateutil.relativedelta.relativedelta(days=1)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def login():   \n",
    "    #登陆 Login\n",
    "    driver.get('https://weibo.cn/pub/?vt=' )\n",
    "   \n",
    "    elem = driver.find_element_by_xpath(\"//*[@class='ut']/a[1]\").click();\n",
    "    time.sleep(1)\n",
    "    \n",
    "    print(\"开始自动登陆Loading\")\n",
    "    elem = driver.find_element_by_xpath(\"//*[@id='loginName']\");\n",
    "    elem.send_keys(username)\n",
    "    elem = driver.find_element_by_xpath(\"//*[@id='loginPassword']\");\n",
    "    elem.send_keys(password)\n",
    "    elem = driver.find_element_by_xpath(\"//*[@id='loginAction']\");\n",
    "    elem.send_keys(Keys.ENTER)  \n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "#每爬完一天清理一次临时数据\n",
    "#clear temp data after each day\n",
    "def clean_temp_data():\n",
    "    usernames.clear()\n",
    "    user_links.clear()\n",
    "    posts.clear()\n",
    "    likes.clear()\n",
    "    reblogs.clear()\n",
    "    comments.clear()\n",
    "    comment_links.clear()\n",
    "    post_date.clear()\n",
    "    post_time.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "#每天保存\n",
    "#save each day\n",
    "def save_data():\n",
    "    data['Posts'] = posts\n",
    "    data['User Names'] = usernames\n",
    "    data['User Links'] = user_links\n",
    "    data['Likes'] = likes\n",
    "    data['Reblogs'] = reblogs\n",
    "    data['Comments'] = comments\n",
    "    data['Comment Links'] = comment_links\n",
    "    data['Post Date'] = post_date\n",
    "    data['Post Time'] = post_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "#翻页 Next Page\n",
    "def click_next_page():\n",
    "    NextPage = driver.find_element_by_xpath(\"//*[@class='pa']/form/div/a[1]\")\n",
    "            #driver.find_element_by_xpath(\"//a[contains(text(),'下页')]\").click()\n",
    "    NextPage.click()\n",
    "    time.sleep(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "#开始爬一页微博。注意elements是可以抓取多个，element只能抓一个\n",
    "#Scraping one page. Notice elements not element. Elements can find multiple element\n",
    "\n",
    "def scrape_one_page():\n",
    "    #找到发微博的用户名和连接\n",
    "    #Get username and href link\n",
    "    elem = driver.find_elements_by_xpath(\"//*[@class='nk']\")\n",
    "    for post in elem:\n",
    "        usernames.append(post.text)\n",
    "        user_links.append(post.get_attribute('href'))\n",
    "\n",
    "    #找到微博内容\n",
    "    #Get post content\n",
    "    elem = driver.find_elements_by_xpath(\"//*[@class='ctt']\")\n",
    "    for post in elem:\n",
    "        posts.append(post.text)\n",
    "\n",
    "    #点赞\n",
    "    #likes\n",
    "    likes_temp = []\n",
    "    elem = driver.find_elements_by_xpath(\"//a[contains(text(),'赞')]\")\n",
    "    for like in elem:\n",
    "        likes_temp.append(like.text)\n",
    "    for like in likes_temp:\n",
    "        like = re.sub(r'\\D', '', like) #去除掉‘赞’这个字只保留数字Delete 'likes' text\n",
    "        likes.append(like)\n",
    "    \n",
    "    #转发评论\n",
    "    #reblogs\n",
    "    reblog_temp = []\n",
    "    elem = driver.find_elements_by_xpath(\"//a[contains(text(),'转发')]\")\n",
    "    for reblog in elem:\n",
    "        reblog_temp.append(reblog.text)\n",
    "    for reblog in reblog_temp:\n",
    "        reblog = re.sub(r'\\D', '', reblog) #保留数字keep the number\n",
    "        reblogs.append(reblog)  \n",
    "\n",
    "    #评论和评论的链接comment and links to comment section\n",
    "    comment_temp = []\n",
    "    elem = driver.find_elements_by_xpath(\"//a[contains(text(),'评论')]\")\n",
    "    for comment in elem:\n",
    "        comment_temp.append(comment.text)\n",
    "        comment_links.append(comment.get_attribute('href'))#评论的链接links to comment\n",
    "    for comment in comment_temp:\n",
    "        comment = re.sub(r'\\D', '', comment) #保留数字keep the number\n",
    "        comments.append(comment)  \n",
    "\n",
    "    #发布日期和时间\n",
    "    elem = driver.find_elements_by_xpath(\"//*[@class='ct']\")\n",
    "    for time in elem:\n",
    "        post_date.append(time.text.split(' ')[0])\n",
    "        post_time.append(time.text.split(' ')[1])\n",
    "        post_date_normal.append()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "#爬一天内的每一页\n",
    "#Scrape every page in a day\n",
    "def scrape_everything_in_a_day(target_page_per_day):    \n",
    "    \n",
    "    #先爬一页再翻页\n",
    "    #scrape a page first then next page\n",
    "    scrape_one_page()\n",
    "    time.sleep(0.5)\n",
    "    \n",
    "    current_page = []\n",
    "    max_page = []\n",
    "    \n",
    "    #当前页数和最大页数\n",
    "    #get current page and max page\n",
    "    elem = driver.find_element_by_xpath(\"//*[@class='pa']\")\n",
    "    elem = elem.text\n",
    "    elem = elem.split('/')\n",
    "    current_page = elem[0]\n",
    "    max_page = elem[1]\n",
    "    current_page = re.sub(r'\\D','',current_page)\n",
    "    max_page = re.sub(r'\\D','',max_page)\n",
    " \n",
    "    #如果超过最大页码，目标页码变为最大页码\n",
    "    if int(max_page) <target_page_per_day:\n",
    "        target_page_per_day = max_page\n",
    "        \n",
    "    for page in range(int(target_page_per_day)):\n",
    "        click_next_page()\n",
    "        scrape_one_page()\n",
    "        time.sleep(0.5)\n",
    "        \n",
    "        elem = driver.find_element_by_xpath(\"//*[@class='pa']\")\n",
    "        elem = elem.text\n",
    "        elem = elem.split('/')\n",
    "        current_page = elem[0]\n",
    "        current_page = re.sub(r'\\D','',current_page)\n",
    "        \n",
    "        #到最大页数的时候停止\n",
    "        #stop when hitting max page\n",
    "        if int(current_page) == int(max_page):\n",
    "            #保存数据，再把临时的清空\n",
    "            save_data()\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来是搜索前后和按照日期循环\n",
    "Below are before and after search and reiterate through dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_search_page_first_time():\n",
    "    #点开搜索页面 Open search tap\n",
    "    driver.find_element_by_xpath(\"//a[contains(text(),'搜索')]\").click()\n",
    "    driver.find_element_by_xpath(\"//a[contains(text(),'高级搜索')]\").click()\n",
    "    time.sleep(0.5)\n",
    "\n",
    "def open_search_page_reiterate():\n",
    "    #点开‘更多’ Open'more'\n",
    "    driver.find_element_by_xpath(\"//a[contains(text(),'更多')]\").click()\n",
    "    time.sleep(0.5)\n",
    "    \n",
    "def keyword_input(key):\n",
    "    #输入关键词 Input Keywords\n",
    "    #我这里是\"同居\" In this case 'cohabitation'\n",
    "    elem = driver.find_element_by_xpath(\"//*[@type='text']\")\n",
    "    elem.send_keys(key)\n",
    "    \n",
    "def date_input(day):\n",
    "    #开始年月日 Start data e.g, 20200101\n",
    "    elem = driver.find_element_by_xpath(\"//*[@name='starttime']\")\n",
    "    elem.clear()\n",
    "    elem.send_keys(day)\n",
    "\n",
    "    #结束年月日一样 End data same\n",
    "    elem = driver.find_element_by_xpath(\"//*[@name='endtime']\")\n",
    "    elem.clear()\n",
    "    elem.send_keys(day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_first_time(key):\n",
    "    #上面def的两个function\n",
    "    open_search_page_first_time()\n",
    "    keyword_input(key)\n",
    "    \n",
    "    #勾选原创 Click original posts\n",
    "    driver.find_element_by_xpath(\"//*[@name='hasori']\").click()\n",
    "    \n",
    "    date_input(days_normal)\n",
    "    \n",
    "    #搜索 Search\n",
    "    driver.find_element_by_xpath(\"//*[@type='submit']\").click()\n",
    "    \n",
    "def search_reiterate(day):\n",
    "    open_search_page_reiterate()\n",
    "    \n",
    "    #爬第二天所以要把第一天删去\n",
    "    days_normal = days_normal[:1]\n",
    "    date_input(day)\n",
    "    \n",
    "    #搜索 Search\n",
    "    driver.find_element_by_xpath(\"//*[@type='submit']\").click()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def login\n",
    "def dates_in_year(target_year)\n",
    "def date_input()\n",
    "def scrape_by_one_page()\n",
    "def next_page_until(target_page_per_day)\n",
    "def search_first_time(key,day)\n",
    "def search_reiterate(day)\n",
    "这些是目前定义过的比较重要的\n",
    "These are important defs so far"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_temp_data()\n",
    "scrape_everything_in_a_day(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Posts</th>\n",
       "      <th>User Names</th>\n",
       "      <th>User Links</th>\n",
       "      <th>Likes</th>\n",
       "      <th>Reblogs</th>\n",
       "      <th>Comments</th>\n",
       "      <th>Comment Links</th>\n",
       "      <th>Post Date</th>\n",
       "      <th>Post Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>:女人失恋以后，听到大家都普遍反映前男友新找的那个姑娘没自己好看，通常都会无比强烈地暗爽。 ...</td>\n",
       "      <td>小黑爱小白</td>\n",
       "      <td>https://weibo.cn/ryxh784533</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>https://weibo.cn/comment/1EgWd?uid=1261131600&amp;...</td>\n",
       "      <td>2010-01-01</td>\n",
       "      <td>22:44:38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>:《恋爱前规则》这部电影拍得一点也不像原著《和空姐同居的日子》那么精彩，这些网络小说改编成电...</td>\n",
       "      <td>肉包儿和猪猪的纪念</td>\n",
       "      <td>https://weibo.cn/comex</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>https://weibo.cn/comment/1EeGI?uid=1403893343&amp;...</td>\n",
       "      <td>2010-01-01</td>\n",
       "      <td>22:18:39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>:不论东西多少，只要是搬家都是大工程啊。我跟同居者聊天，打算在家里放一块匾，上书“弱智儿童欢...</td>\n",
       "      <td>小壹</td>\n",
       "      <td>https://weibo.cn/u/1657282170</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>https://weibo.cn/comment/1Edu3?uid=1657282170&amp;...</td>\n",
       "      <td>2010-01-01</td>\n",
       "      <td>22:04:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>:看了恋爱前规则，对小说产生了兴趣，《和空姐同居的日子》找出来看，刚刚看完，无限yy</td>\n",
       "      <td>成熟孩子气</td>\n",
       "      <td>https://weibo.cn/lcj0431046</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>https://weibo.cn/comment/1EbVw?uid=1644651464&amp;...</td>\n",
       "      <td>2010-01-01</td>\n",
       "      <td>21:45:27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>:@claireg 同居也是一种选择。</td>\n",
       "      <td>阿力克狮</td>\n",
       "      <td>https://weibo.cn/amour</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>https://weibo.cn/comment/1EaV7?uid=1113519487&amp;...</td>\n",
       "      <td>2010-01-01</td>\n",
       "      <td>21:33:31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>:@thales 提倡合法同居。</td>\n",
       "      <td>阿力克狮</td>\n",
       "      <td>https://weibo.cn/amour</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>https://weibo.cn/comment/1EaJ4?uid=1113519487&amp;...</td>\n",
       "      <td>2010-01-01</td>\n",
       "      <td>21:30:53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>:@thales 你就忍心人家下半辈子活在坟墓之中？撕掉本本，还可以同居嘛。</td>\n",
       "      <td>阿力克狮</td>\n",
       "      <td>https://weibo.cn/amour</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>https://weibo.cn/comment/1Ear6?uid=1113519487&amp;...</td>\n",
       "      <td>2010-01-01</td>\n",
       "      <td>21:26:55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>:(1/2)知道周迅的越来越多，知道鲁迅的越来越少；知道马克的越来越多，知道马克思的越来越少...</td>\n",
       "      <td>i幸福i晓林</td>\n",
       "      <td>https://weibo.cn/chinalunyu</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>https://weibo.cn/comment/1DLOh?uid=1656366057&amp;...</td>\n",
       "      <td>2010-01-01</td>\n",
       "      <td>14:27:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>:如果爱一个人，千万不要与他同居或是结婚。 维持一个辽阔的距离，偶遇，可以爱慕的目光致敬， ...</td>\n",
       "      <td>田田的牧歌</td>\n",
       "      <td>https://weibo.cn/u/1662934211</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>https://weibo.cn/comment/1DDuL?uid=1662934211&amp;...</td>\n",
       "      <td>2010-01-01</td>\n",
       "      <td>11:53:24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>:刚刚看完小说《和空姐同居的日子》。感觉非常好，我觉得虽然有点脱离实际，但不可否认的是它的故...</td>\n",
       "      <td>神无月琉璃</td>\n",
       "      <td>https://weibo.cn/u/1651816812</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>https://weibo.cn/comment/1DtU5?uid=1651816812&amp;...</td>\n",
       "      <td>2010-01-01</td>\n",
       "      <td>08:53:49</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Posts User Names  \\\n",
       "0  :女人失恋以后，听到大家都普遍反映前男友新找的那个姑娘没自己好看，通常都会无比强烈地暗爽。 ...      小黑爱小白   \n",
       "1  :《恋爱前规则》这部电影拍得一点也不像原著《和空姐同居的日子》那么精彩，这些网络小说改编成电...  肉包儿和猪猪的纪念   \n",
       "2  :不论东西多少，只要是搬家都是大工程啊。我跟同居者聊天，打算在家里放一块匾，上书“弱智儿童欢...         小壹   \n",
       "3         :看了恋爱前规则，对小说产生了兴趣，《和空姐同居的日子》找出来看，刚刚看完，无限yy      成熟孩子气   \n",
       "4                                :@claireg 同居也是一种选择。       阿力克狮   \n",
       "5                                   :@thales 提倡合法同居。       阿力克狮   \n",
       "6             :@thales 你就忍心人家下半辈子活在坟墓之中？撕掉本本，还可以同居嘛。       阿力克狮   \n",
       "7  :(1/2)知道周迅的越来越多，知道鲁迅的越来越少；知道马克的越来越多，知道马克思的越来越少...     i幸福i晓林   \n",
       "8  :如果爱一个人，千万不要与他同居或是结婚。 维持一个辽阔的距离，偶遇，可以爱慕的目光致敬， ...      田田的牧歌   \n",
       "9  :刚刚看完小说《和空姐同居的日子》。感觉非常好，我觉得虽然有点脱离实际，但不可否认的是它的故...      神无月琉璃   \n",
       "\n",
       "                      User Links Likes Reblogs Comments  \\\n",
       "0    https://weibo.cn/ryxh784533     0       0        0   \n",
       "1         https://weibo.cn/comex     0       0        0   \n",
       "2  https://weibo.cn/u/1657282170     0       0        2   \n",
       "3    https://weibo.cn/lcj0431046     0       0        0   \n",
       "4         https://weibo.cn/amour     0       0        0   \n",
       "5         https://weibo.cn/amour     0       0        0   \n",
       "6         https://weibo.cn/amour     0       0        0   \n",
       "7    https://weibo.cn/chinalunyu     0       0        0   \n",
       "8  https://weibo.cn/u/1662934211     0       0        0   \n",
       "9  https://weibo.cn/u/1651816812     0       0        0   \n",
       "\n",
       "                                       Comment Links   Post Date Post Time  \n",
       "0  https://weibo.cn/comment/1EgWd?uid=1261131600&...  2010-01-01  22:44:38  \n",
       "1  https://weibo.cn/comment/1EeGI?uid=1403893343&...  2010-01-01  22:18:39  \n",
       "2  https://weibo.cn/comment/1Edu3?uid=1657282170&...  2010-01-01  22:04:23  \n",
       "3  https://weibo.cn/comment/1EbVw?uid=1644651464&...  2010-01-01  21:45:27  \n",
       "4  https://weibo.cn/comment/1EaV7?uid=1113519487&...  2010-01-01  21:33:31  \n",
       "5  https://weibo.cn/comment/1EaJ4?uid=1113519487&...  2010-01-01  21:30:53  \n",
       "6  https://weibo.cn/comment/1Ear6?uid=1113519487&...  2010-01-01  21:26:55  \n",
       "7  https://weibo.cn/comment/1DLOh?uid=1656366057&...  2010-01-01  14:27:57  \n",
       "8  https://weibo.cn/comment/1DDuL?uid=1662934211&...  2010-01-01  11:53:24  \n",
       "9  https://weibo.cn/comment/1DtU5?uid=1651816812&...  2010-01-01  08:53:49  "
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.DataFrame(data)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始自动登陆，若出现验证码手动验证.Loading\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'max_page' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-110-e6cc93cbd26a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0mpassword\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"rw297421\"\u001b[0m \u001b[1;31m#密码password\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[0mdriver\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwebdriver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mChrome\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;31m#可以换成firefox.Can use firefox\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m     \u001b[0mscrape_by_date\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2010\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[1;31m#页数可以设置成max_page\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-110-e6cc93cbd26a>\u001b[0m in \u001b[0;36mscrape_by_date\u001b[1;34m(target_year, target_page_per_day)\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mdate\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdays_normal\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[0mscrape_one_page\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m         \u001b[0mnext_page_until\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget_page_per_day\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m         \u001b[0mdate_input_and_search_reiterate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'爬完'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mdate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-107-c75b70fd6d5a>\u001b[0m in \u001b[0;36mnext_page_until\u001b[1;34m(target_page_per_day)\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[0mtarget_page_per_day\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget_page_per_day\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0mtarget_page_per_day\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mmax_page\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'超出最大页数，改为爬最大页数'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mmax_page\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Over page limit. Instead, scrape to page limit'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mmax_page\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m: local variable 'max_page' referenced before assignment"
     ]
    }
   ],
   "source": [
    "def scrape_by_date(key, target_year, target_page_per_day):\n",
    "    login()\n",
    "    dates_in_year(target_year)\n",
    "    search_first_time(key)\n",
    "    for day in dates_normal:\n",
    "        search_reiterate(key,day)\n",
    "        for day in days_normal[:1]:\n",
    "            clean_temp_data()\n",
    "            scrape_everything_in_a_day(target_page_per_day)\n",
    "            date_input_and_search(day)\n",
    "            print('Finish' + day)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    username = \"17064888409\" #用户名username\n",
    "    password = \"rw297421\" #密码password\n",
    "    driver = webdriver.Chrome()#可以换成firefox.Can use firefox        \n",
    "    scrape_by_date('同居',2010, 10)\n",
    "#页数可以设置成max_page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_by_date(key, target_year, target_page_per_day):\n",
    "    login()\n",
    "    dates_in_year(target_year)\n",
    "    search_first_time(key)\n",
    "    for day in dates_normal:\n",
    "        search_reiterate(key,day)\n",
    "        for day in days_normal[:1]:\n",
    "            scrape_one_page()\n",
    "            next_page_until(target_page_per_day)\n",
    "            date_input_and_search(day)\n",
    "            print('Finish' + day)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    username = \"17064888409\" #用户名username\n",
    "    password = \"rw297421\" #密码password\n",
    "    driver = webdriver.Chrome()#可以换成firefox.Can use firefox        \n",
    "    scrape_by_date('同居',2010, 10)\n",
    "#页数可以设置成max_page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始自动登陆Loading\n"
     ]
    }
   ],
   "source": [
    "def scrape_by_date(key, target_year, target_page_per_day):\n",
    "    login()\n",
    "    dates_in_year(target_year)\n",
    "    search_first_time(key)\n",
    "    for day in dates_normal:\n",
    "        search_reiterate(key,day)\n",
    "        for day in days_normal[:1]:\n",
    "            scrape_one_page()\n",
    "            next_page_until(target_page_per_day)\n",
    "            date_input_and_search(day)\n",
    "            print('Finish' + day)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    username = \"17064888409\" #用户名username\n",
    "    password = \"rw297421\" #密码password\n",
    "    driver = webdriver.Chrome()#可以换成firefox.Can use firefox        \n",
    "    scrape_by_date('同居',2010, 10)\n",
    "#页数可以设置成max_page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
