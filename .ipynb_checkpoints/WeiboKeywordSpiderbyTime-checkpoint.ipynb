{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import xlrd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "import datetime\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "import dateutil.relativedelta\n",
    "import calendar\n",
    "import re\n",
    "import pandas\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "前面都是def funciton，最后把所有function汇集起来\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "我手头的项目是要爬2010年到现在每个月/每一天的关于同居的微博。目前常见的微博爬虫有三个目标网站，一个是pc端口s.weibo.com，一个是智能手机端口m.weibo.cn，一个是非常老的手机端口weibo.cn。很多微博爬虫都采用智能手机端端口，因为可以爬到很多数据，只要不停往下滑应该就能爬完。但是我目前还没有发现智能手机端口如何搜索指定的日期或时间段，这就不得不用pc端口或者非常老的那个手机端口了。\n",
    "\n",
    "非常老的手机端口的好处在于不用管验证码，直接就可以登陆，省去了非常多的麻烦。而且不存在‘展开全文’的问题，pc端和智能手机端需要展开原文。而且每一个日期可以搜索到100页，pc端只能搜索到50页。虽然可能pc端每一页比手机端口要多，但是应该还是这个端口能爬到的更多一点。还有一个原因是可以直接勾选原创微博，省去了看上去爬了很多，但实际上爬完了还需要删除所有的转发的过程（虽然复制粘贴的还是要额外删除）。\n",
    "\n",
    "用selenium是因为微博会反爬虫，selenium可以模拟正常登陆的流程，而且一定要在每一次点击一个页面之后停顿几秒钟（sleep），不然很容易被封号甚至封IP。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#我本来想按照月来爬的，但是输入指定月份例如20100101-20100131之后\n",
    "#爬到的几乎都是1月31号的微博，这样的样本不是很好\n",
    "#所以还是每天都爬2、3页当做样本，过几个星期我再每天都爬100页试试\n",
    "\n",
    "def dates_in_year(YEAR):\n",
    "    YEAR = int(YEAR)\n",
    "    MONTH = 1\n",
    "    DAY = 1\n",
    "    date = datetime(YEAR,MONTH,DAY)\n",
    "    \n",
    "    days_in_year = 365 + (1*calendar.isleap(YEAR))\n",
    "    for i in range(days_in_year):\n",
    "        days_normal.append(date.strftime(\"%Y\")+date.strftime(\"%m\")+date.strftime(\"%d\"))\n",
    "        days_datetime.append(date) \n",
    "        date = date + dateutil.relativedelta.relativedelta(days=1)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def login():   \n",
    "    #登陆 Login\n",
    "    driver.get('https://weibo.cn/pub/?vt=' )\n",
    "   \n",
    "    elem = driver.find_element_by_xpath(\"//*[@class='ut']/a[1]\").click();\n",
    "    time.sleep(1)\n",
    "    \n",
    "    print(\"开始自动登陆Loading\")\n",
    "    elem = driver.find_element_by_xpath(\"//*[@id='loginName']\");\n",
    "    elem.send_keys(username)\n",
    "    elem = driver.find_element_by_xpath(\"//*[@id='loginPassword']\");\n",
    "    elem.send_keys(password)\n",
    "    elem = driver.find_element_by_xpath(\"//*[@id='loginAction']\");\n",
    "    elem.send_keys(Keys.ENTER)  \n",
    "    time.sleep(1)\n",
    "    print('登陆完毕 Login Finish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#每爬完一天清理一次临时数据\n",
    "#clear temp data after each day\n",
    "def clean_temp_data():\n",
    "    usernames.clear()\n",
    "    user_links.clear()\n",
    "    posts.clear()\n",
    "    likes.clear()\n",
    "    reblogs.clear()\n",
    "    comments.clear()\n",
    "    comment_links.clear()\n",
    "    post_date.clear()\n",
    "    post_time.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#每天保存\n",
    "#save each day\n",
    "def save_data():\n",
    "    for post in posts:\n",
    "        data['Posts'].append(post)\n",
    "    for username in usernames:\n",
    "        data['User Names'].append(username)\n",
    "    for user_link in user_links:\n",
    "        data['User Links'].append(user_link)\n",
    "    for like in likes:\n",
    "        data['Likes'].append(like)\n",
    "    for reblog in reblogs:\n",
    "        data['Reblogs'].append(reblog)\n",
    "    for comment in comments:\n",
    "        data['Comments'].append(comment)\n",
    "    for comment_link in comment_links:\n",
    "        data['Comment Links'].append(comment_link)\n",
    "    for post_d in post_date:\n",
    "        data['Post Date'].append(post_d)\n",
    "    for post_t in post_time:\n",
    "        data['Post Time'].append(post_t)\n",
    "    for post_date_d in post_date_datetime:\n",
    "        data['Date in Datetime Format'].append(post_date_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#翻页 Next Page\n",
    "def click_next_page():\n",
    "    NextPage = driver.find_element_by_xpath(\"//*[@class='pa']/form/div/a[1]\")\n",
    "            #driver.find_element_by_xpath(\"//a[contains(text(),'下页')]\").click()\n",
    "    NextPage.click()\n",
    "    time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#开始爬一页微博。注意elements是可以抓取多个，element只能抓一个\n",
    "#Scraping one page. Notice elements not element. Elements can find multiple element\n",
    "\n",
    "def scrape_one_page():\n",
    "    #找到发微博的用户名和连接\n",
    "    #Get username and href link\n",
    "    elem = driver.find_elements_by_xpath(\"//*[@class='nk']\")\n",
    "    for post in elem:\n",
    "        usernames.append(post.text)\n",
    "        user_links.append(post.get_attribute('href'))\n",
    "\n",
    "    #找到微博内容\n",
    "    #Get post content\n",
    "    elem = driver.find_elements_by_xpath(\"//*[@class='ctt']\")\n",
    "    for post in elem:\n",
    "        posts.append(post.text)\n",
    "\n",
    "    #点赞\n",
    "    #likes\n",
    "    likes_temp = []\n",
    "    elem = driver.find_elements_by_xpath(\"//a[contains(text(),'赞')]\")\n",
    "    for like in elem:\n",
    "        likes_temp.append(like.text)\n",
    "    for like in likes_temp:\n",
    "        like = re.sub(r'\\D', '', like) #去除掉‘赞’这个字只保留数字Delete 'likes' text\n",
    "        likes.append(like)\n",
    "    \n",
    "    #转发评论\n",
    "    #reblogs\n",
    "    reblog_temp = []\n",
    "    elem = driver.find_elements_by_xpath(\"//a[contains(text(),'转发')]\")\n",
    "    for reblog in elem:\n",
    "        reblog_temp.append(reblog.text)\n",
    "    for reblog in reblog_temp:\n",
    "        reblog = re.sub(r'\\D', '', reblog) #保留数字keep the number\n",
    "        reblogs.append(reblog)  \n",
    "\n",
    "    #评论和评论的链接comment and links to comment section\n",
    "    comment_temp = []\n",
    "    elem = driver.find_elements_by_xpath(\"//a[contains(text(),'评论')]\")\n",
    "    for comment in elem:\n",
    "        comment_temp.append(comment.text)\n",
    "        comment_links.append(comment.get_attribute('href'))#评论的链接links to comment\n",
    "    for comment in comment_temp:\n",
    "        comment = re.sub(r'\\D', '', comment) #保留数字keep the number\n",
    "        comments.append(comment)  \n",
    "\n",
    "    #发布日期和时间\n",
    "    elem = driver.find_elements_by_xpath(\"//*[@class='ct']\")\n",
    "    for time in elem:\n",
    "        post_date.append(time.text.split(' ')[0])\n",
    "        post_time.append(time.text.split(' ')[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#爬一天内的每一页\n",
    "#Scrape every page in a day\n",
    "def scrape_everything_in_a_day(target_page_per_day):    \n",
    "    \n",
    "    current_page = []\n",
    "    max_page = []\n",
    "    \n",
    "    #当前页数和最大页数\n",
    "    #get current page and max page\n",
    "    try:\n",
    "        elem = driver.find_element_by_xpath(\"//*[@class='pa']\")\n",
    "    except NoSuchElementException:\n",
    "            driver.back()\n",
    "            time.sleep(5)\n",
    "            driver.find_element_by_xpath(\"//*[@type='submit']\").click()\n",
    "            elem = driver.find_element_by_xpath(\"//*[@class='pa']\")\n",
    "    elem = elem.text\n",
    "    elem = elem.split('/')\n",
    "    current_page = elem[0]\n",
    "    max_page = elem[1]\n",
    "    current_page = re.sub(r'\\D','',current_page)\n",
    "    max_page = re.sub(r'\\D','',max_page)\n",
    " \n",
    "    #如果超过最大页码，目标页码变为最大页码\n",
    "    if int(max_page) <target_page_per_day:\n",
    "        target_page = max_page\n",
    "\n",
    "    #开始循环, 如果被限制的话就返回停五秒钟再前进。   \n",
    "    for page in range(int(target_page)):\n",
    "        try:\n",
    "            scrape_one_page()\n",
    "            time.sleep(1)\n",
    "        except NoSuchElementException:\n",
    "            driver.back()\n",
    "            time.sleep(5)\n",
    "            try:\n",
    "                click_next_page() #第一种情况点击下一页\n",
    "            except NoSuchElementException:\n",
    "                driver.find_element_by_xpath(\"//*[@type='submit']\").click()\n",
    "                #第二种情况点击搜索\n",
    "            \n",
    "            \n",
    "        #有一些页面是空的所以用try except跳过去防止报错\n",
    "        try:\n",
    "            elem = driver.find_element_by_xpath(\"//*[@class='pa']\")\n",
    "            elem = elem.text\n",
    "            elem = elem.split('/')\n",
    "            current_page = elem[0]\n",
    "            current_page = re.sub(r'\\D','',current_page)\n",
    "            #到最大页数的时候停止\n",
    "            #stop when hitting max page\n",
    "            if int(current_page) == int(max_page):\n",
    "                #保存数据\n",
    "                save_data()\n",
    "                break\n",
    "            else:\n",
    "                click_next_page()\n",
    "    \n",
    "        except NoSuchElementException:\n",
    "            driver.back()\n",
    "            time.sleep(5)\n",
    "            driver.find_element_by_xpath(\"//*[@type='submit']\").click()\n",
    "           \n",
    "            elem = driver.find_element_by_xpath(\"//*[@class='pa']\")\n",
    "            elem = elem.text\n",
    "            elem = elem.split('/')\n",
    "            current_page = elem[0]\n",
    "            current_page = re.sub(r'\\D','',current_page)\n",
    "        \n",
    "            if int(current_page) == int(max_page)-1:\n",
    "                #保存数据\n",
    "                save_data()\n",
    "                break\n",
    "            else:\n",
    "                try:\n",
    "                    elem = driver.find_element_by_xpath(\"//*[@input='page']\")\n",
    "                except NoSuchElementException:\n",
    "                    elem = driver.find_element_by_xpath(\"//*[@name='page']\")\n",
    "                jump_one_page = int(current_page) + 2\n",
    "                elem.send_keys(jump_one_page)\n",
    "                driver.find_element_by_xpath(\"//*[@value='跳页']\").click()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来是搜索前后和按照日期循环\n",
    "\n",
    "Below are before and after search and reiterate through dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_search_page_first_time():\n",
    "    #点开搜索页面 Open search tap\n",
    "    driver.find_element_by_xpath(\"//a[contains(text(),'搜索')]\").click()\n",
    "    driver.find_element_by_xpath(\"//a[contains(text(),'高级搜索')]\").click()\n",
    "    time.sleep(1)\n",
    "    \n",
    "def keyword_input(key):\n",
    "    #输入关键词 Input Keywords\n",
    "    #我这里是\"同居\" In this case 'cohabitation'\n",
    "    elem = driver.find_element_by_xpath(\"//*[@type='text']\")\n",
    "    elem.send_keys(key)\n",
    "    \n",
    "def date_input(day):\n",
    "    #开始年月日 Start data e.g, 20200101\n",
    "    elem = driver.find_element_by_xpath(\"//*[@name='starttime']\")\n",
    "    elem.clear()\n",
    "    elem.send_keys(day)\n",
    "\n",
    "    #结束年月日一样 End data same\n",
    "    elem = driver.find_element_by_xpath(\"//*[@name='endtime']\")\n",
    "    elem.clear()\n",
    "    elem.send_keys(day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_first_time(key):\n",
    "    #上面def的两个function\n",
    "    open_search_page_first_time()\n",
    "    keyword_input(key)\n",
    "    \n",
    "    #勾选原创 Click original posts\n",
    "    driver.find_element_by_xpath(\"//*[@name='hasori']\").click()\n",
    "    \n",
    "    date_input(days_normal[0])\n",
    "    \n",
    "    #搜索 Search\n",
    "    driver.find_element_by_xpath(\"//*[@type='submit']\").click()\n",
    "    time.sleep(1)\n",
    "    \n",
    "def search_reiterate(day):\n",
    "    #点开‘更多’ Open'more'\n",
    "    #因为之前输入过关键词和勾选原创，这里可以省略\n",
    "    #Because we have input keywords and click original, we can skip here\n",
    "    driver.find_element_by_xpath(\"//a[contains(text(),'更多')]\").click()\n",
    "    time.sleep(2)\n",
    "    \n",
    "    date_input(day)\n",
    "    \n",
    "    #搜索 Search\n",
    "    driver.find_element_by_xpath(\"//*[@type='submit']\").click()\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def login\n",
    "def dates_in_year(target_year)\n",
    "def date_input()\n",
    "def scrape_by_one_page()\n",
    "def next_page_until(target_page_per_day)\n",
    "def search_first_time(key,day)\n",
    "def search_reiterate(day)\n",
    "\n",
    "\n",
    "这些是目前定义过的比较重要的，接下来把他们拼起来就行\n",
    "These are important defs so far. We just need to put them together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_by_date(key, target_year, target_page_per_day):\n",
    "    \n",
    "    #创建指定年份的日期列表\n",
    "    #Create a list of dyas in target year\n",
    "    dates_in_year(target_year)\n",
    "    \n",
    "    #登陆\n",
    "    #login\n",
    "    login()\n",
    "    \n",
    "    #开始搜索指定关键词的第一天的原创微博\n",
    "    #search for keywords, click original, on the first day in the list\n",
    "    search_first_time(key)\n",
    "    \n",
    "    #爬+保存，清除临时数据列表\n",
    "    scrape_everything_in_a_day(target_page_per_day)\n",
    "    for post in range(len(posts)):\n",
    "        post_date_datetime.append(days_datetime[0])\n",
    "    clean_temp_data()\n",
    "    \n",
    "    #从第二天开始循环，所以去掉列表里第一天\n",
    "    #Reit from second day, so delete first day in the list\n",
    "    days_normal_from_2nd_day = []\n",
    "    days_normal_from_2nd_day = days_normal[1:]\n",
    "    for day in days_normal_from_2nd_day:\n",
    "        search_reiterate(day)\n",
    "        scrape_everything_in_a_day(target_page_per_day)\n",
    "        for post in range(len(posts)):\n",
    "            post_date_datetime.append(datetime(int(day[:4]),\n",
    "                                             int(day[4:6]),\n",
    "                                             int(day[6:8])))\n",
    "        print('Finish ' + day)\n",
    "        clean_temp_data()\n",
    "    \n",
    "    print('Finish scraping ' + key + 'in ' + target_year + 'for '\n",
    "         + target_page_per_day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始自动登陆Loading\n",
      "登陆完毕 Login Finish\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'post_date_datetimg' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-6e7ec544e108>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[0mpassword\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"rw297421\"\u001b[0m \u001b[1;31m#密码password\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[0mdriver\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwebdriver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mChrome\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;31m#可以换成firefox.Can use firefox\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m     \u001b[0mscrape_by_date\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'同居'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2010\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m \u001b[1;31m#页数可以设置成max_page\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-11-840e7b0d5c68>\u001b[0m in \u001b[0;36mscrape_by_date\u001b[1;34m(key, target_year, target_page_per_day)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[1;31m#爬+保存，清除临时数据列表\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m     \u001b[0mscrape_everything_in_a_day\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget_page_per_day\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mpost\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mposts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[0mpost_date_datetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdays_datetime\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-8-08c97abc69ff>\u001b[0m in \u001b[0;36mscrape_everything_in_a_day\u001b[1;34m(target_page_per_day)\u001b[0m\n\u001b[0;32m     52\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcurrent_page\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_page\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m                 \u001b[1;31m#保存数据\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m                 \u001b[0msave_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     55\u001b[0m                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-df7f754a1c09>\u001b[0m in \u001b[0;36msave_data\u001b[1;34m()\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mpost_t\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpost_time\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Post Time'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpost_t\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mpost_date_d\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpost_date_datetimg\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m         \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Date in Datetime Format'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpost_date_d\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'post_date_datetimg' is not defined"
     ]
    }
   ],
   "source": [
    "#创建临时数据列表\n",
    "days_normal = []\n",
    "days_datetime = []\n",
    "usernames = []\n",
    "user_links = []\n",
    "posts = []\n",
    "likes = []\n",
    "reblogs = []\n",
    "comments = []\n",
    "comment_links = []\n",
    "post_date = []   \n",
    "post_time = []\n",
    "post_date_datetime = []\n",
    "data = {'Posts':[],\n",
    "    'User Names':[],\n",
    "    'User Links':[],\n",
    "    'Likes':[],\n",
    "    'Reblogs':[],\n",
    "    'Comments':[],\n",
    "    'Comment Links':[],\n",
    "    'Post Date':[],\n",
    "    'Post Time':[],\n",
    "    'Date in Datetime Format':[]}\n",
    "\n",
    "\n",
    "#正式跑起来\n",
    "if __name__ == '__main__':\n",
    "    username = \"17064888409\" #用户名username\n",
    "    password = \"rw297421\" #密码password\n",
    "    driver = webdriver.Chrome()#可以换成firefox.Can use firefox        \n",
    "    scrape_by_date('同居', 2010,  5)\n",
    "#页数可以设置成max_page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data['Posts'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data['User Names'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['User Names']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['User Names']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
