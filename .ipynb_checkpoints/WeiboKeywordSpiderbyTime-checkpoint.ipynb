{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import xlrd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import datetime\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "import dateutil.relativedelta\n",
    "import calendar\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我手头的项目是要爬2010年到现在每个月/每一天的关于同居的微博。目前常见的微博爬虫有三个目标网站，一个是pc端口s.weibo.com，一个是智能手机端口m.weibo.cn，一个是非常老的手机端口weibo.cn。很多微博爬虫都采用智能手机端端口，因为可以爬到很多数据，只要不停往下滑应该就能爬完。但是我目前还没有发现智能手机端口如何搜索指定的日期或时间段，这就不得不用pc端口或者非常老的那个手机端口了。\n",
    "\n",
    "非常老的手机端口的好处在于不用管验证码，直接就可以登陆，省去了非常多的麻烦。而且不存在‘展开全文’的问题，pc端和智能手机端需要展开原文。而且每一个日期可以搜索到100页，pc端只能搜索到50页。虽然可能pc端每一页比手机端口要多，但是应该还是这个端口能爬到的更多一点。还有一个原因是可以直接勾选原创微博，省去了看上去爬了很多，但实际上爬完了还需要删除所有的转发的过程（虽然复制粘贴的还是要额外删除）。\n",
    "\n",
    "用selenium是因为微博会反爬虫，selenium可以模拟正常登陆的流程，而且一定要在每一次点击一个页面之后停顿几秒钟（sleep），不然很容易被封号甚至封IP。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "days_normal = []\n",
    "days_datetime = []\n",
    "usernames = []\n",
    "user_links = []\n",
    "posts = []\n",
    "likes = []\n",
    "reblogs = []\n",
    "comments = []\n",
    "comment_links = []\n",
    "post_date = []\n",
    "post_time = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#我本来想按照月来爬的，但是输入指定月份例如20100101-20100131之后\n",
    "#爬到的几乎都是1月31号的微博，这样的样本不是很好\n",
    "#所以还是每天都爬2、3页当做样本，过几个星期我再每天都爬100页试试\n",
    "\n",
    "def dates_in_year(YEAR):\n",
    "    YEAR = int(YEAR)\n",
    "    MONTH = 1\n",
    "    DAY = 1\n",
    "    date = datetime(YEAR,MONTH,DAY)\n",
    "    \n",
    "    days_in_year = 365 + (1*calendar.isleap(YEAR))\n",
    "    for i in range(days_in_year):\n",
    "        days_normal.append(date.strftime(\"%Y\")+date.strftime(\"%m\")+date.strftime(\"%d\"))\n",
    "        days_datetime.append(date) \n",
    "        date = date + dateutil.relativedelta.relativedelta(days=1)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def login():   \n",
    "    #登陆 Login\n",
    "    driver.get('https://weibo.cn/pub/?vt=' )\n",
    "   \n",
    "    elem = driver.find_element_by_xpath(\"//*[@class='ut']/a[1]\").click();\n",
    "    time.sleep(1)\n",
    "    \n",
    "    print(\"开始自动登陆Loading\")\n",
    "    elem = driver.find_element_by_xpath(\"//*[@id='loginName']\");\n",
    "    elem.send_keys(username)\n",
    "    elem = driver.find_element_by_xpath(\"//*[@id='loginPassword']\");\n",
    "    elem.send_keys(password)\n",
    "    elem = driver.find_element_by_xpath(\"//*[@id='loginAction']\");\n",
    "    elem.send_keys(Keys.ENTER)  \n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始自动登陆Loading\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    username = \"17064888409\" #用户名username\n",
    "    password = \"rw297421\" #密码password\n",
    "    driver = webdriver.Chrome()#可以换成firefox.Can use firefox\n",
    "    login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_search_page_first_time():\n",
    "    #点开搜索页面 Open search tap\n",
    "    driver.find_element_by_xpath(\"//a[contains(text(),'搜索')]\").click()\n",
    "    driver.find_element_by_xpath(\"//a[contains(text(),'高级搜索')]\").click()\n",
    "    time.sleep(0.5)\n",
    "\n",
    "def open_search_page_reiterate():\n",
    "    #点开‘更多’ Open'more'\n",
    "    driver.find_element_by_xpath(\"//a[contains(text(),'更多')]\").click()\n",
    "    time.sleep(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keyword_input(key):\n",
    "    #输入关键词 Input Keywords\n",
    "    #我这里是\"同居\" In this case 'cohabitation'\n",
    "    elem = driver.find_element_by_xpath(\"//*[@type='text']\")\n",
    "    elem.send_keys(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def date_input(day):\n",
    "    #开始年月日 Start data e.g, 20200101\n",
    "    elem = driver.find_element_by_xpath(\"//*[@name='starttime']\")\n",
    "    elem.clear()\n",
    "    elem.send_keys(day)\n",
    "\n",
    "    #结束年月日一样 End data same\n",
    "    elem = driver.find_element_by_xpath(\"//*[@name='endtime']\")\n",
    "    elem.clear()\n",
    "    elem.send_keys(day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_first_time(key,day):\n",
    "    #上面def的两个function\n",
    "    open_search_page_first_time()\n",
    "    keyword_input(key)\n",
    "    \n",
    "    #勾选原创 Click original posts\n",
    "    driver.find_element_by_xpath(\"//*[@name='hasori']\").click()\n",
    "    \n",
    "    date_input(day)\n",
    "    \n",
    "    #搜索 Search\n",
    "    driver.find_element_by_xpath(\"//*[@type='submit']\").click()\n",
    "    \n",
    "def search_reiterate(day):\n",
    "    open_search_page_reiterate()\n",
    "    \n",
    "    #爬第二天所以要把第一天删去\n",
    "    days_normal = days_normal[:1]\n",
    "    date_input(day)\n",
    "    \n",
    "    #搜索 Search\n",
    "    driver.find_element_by_xpath(\"//*[@type='submit']\").click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "#开始爬微博scraping,注意elements是可以抓取多个，element只能抓一个\n",
    "#start scraping. Notice elements not element. Elements can find multiple element\n",
    "\n",
    "def scrape_one_page():\n",
    "    #找到发微博的用户名和连接\n",
    "    #Get username and href link\n",
    "    elem = driver.find_elements_by_xpath(\"//*[@class='nk']\")\n",
    "    for post in elem:\n",
    "        usernames.append(post.text)\n",
    "        user_links.append(post.get_attribute('href'))\n",
    "\n",
    "    #找到微博内容\n",
    "    #Get post content\n",
    "    elem = driver.find_elements_by_xpath(\"//*[@class='ctt']\")\n",
    "    for post in elem:\n",
    "        posts.append(post.text)\n",
    "\n",
    "    #点赞\n",
    "    #likes\n",
    "    likes_temp = []\n",
    "    elem = driver.find_elements_by_xpath(\"//a[contains(text(),'赞')]\")\n",
    "    for like in elem:\n",
    "        likes_temp.append(like.text)\n",
    "    for like in likes_temp:\n",
    "        like = re.sub(r'\\D', '', like) #去除掉‘赞’这个字只保留数字Delete 'likes' text\n",
    "        likes.append(like)\n",
    "    \n",
    "    #转发评论\n",
    "    #reblogs\n",
    "    reblog_temp = []\n",
    "    elem = driver.find_elements_by_xpath(\"//a[contains(text(),'转发')]\")\n",
    "    for reblog in elem:\n",
    "        reblog_temp.append(reblog.text)\n",
    "    for reblog in reblog_temp:\n",
    "        reblog = re.sub(r'\\D', '', reblog) #保留数字keep the number\n",
    "        reblogs.append(reblog)  \n",
    "\n",
    "    #评论和评论的链接comment and links to comment section\n",
    "    comment_temp = []\n",
    "    elem = driver.find_elements_by_xpath(\"//a[contains(text(),'评论')]\")\n",
    "    for comment in elem:\n",
    "        comment_temp.append(comment.text)\n",
    "        comment_links.append(comment.get_attribute('href'))#评论的链接links to comment\n",
    "    for comment in comment_temp:\n",
    "        comment = re.sub(r'\\D', '', comment) #保留数字keep the number\n",
    "        comments.append(comment)  \n",
    "\n",
    "    #发布日期和时间\n",
    "    post_date = []\n",
    "    elem = driver.find_elements_by_xpath(\"//*[@class='ct']\")\n",
    "    for time in elem:\n",
    "        post_date.append(time.text.split(' ')[0])\n",
    "        post_time.append(time.text.split(' ')[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "#一页爬完了爬下一页Next page\n",
    "#driver.find_element_by_xpath(\"//a[contains(text(),'下页')]\").click()\n",
    "\n",
    "def click_next_page():\n",
    "    NextPage = driver.find_element_by_xpath(\"//*[@class='pa']/form/div/a[1]\")\n",
    "            #driver.find_element_by_xpath(\"//a[contains(text(),'下页')]\").click()\n",
    "    NextPage.click()\n",
    "    time.sleep(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "#翻页加爬直到指定页面Next page and scrape until target page\n",
    "def next_page_until(target_page_per_day):\n",
    "    #找到当前页码和页码上限\n",
    "    #find current page number and max page\n",
    "    elem = driver.find_elements_by_xpath(\"//*[@class='pa']\")\n",
    "    for page in elem:\n",
    "        page = page.text.split(' ')[4:]\n",
    "        for p in page:\n",
    "            page_number = p[:-1]\n",
    "            page_number = page_number.split('/')\n",
    "            current_page = int(page_number[0])\n",
    "            max_page = int(page_number[1])\n",
    "            \n",
    "    #判断输入MAX还是指定页数，不能超过100\n",
    "    #Input MAX or numbers, less than 100\n",
    "    if target_page_per_day == 'MAX':\n",
    "        target_page_per_day = max_page\n",
    "    else:\n",
    "        target_page_per_day = int(target_page_per_day)\n",
    "        if target_page_per_day > max_page:\n",
    "            print('超出最大页数，改为爬最大页数' + max_page)\n",
    "            print('Over page limit. Instead, scrape to page limit' + max_page)\n",
    "        \n",
    "    #在目标页数范围内爬，在达到指定页面前翻页\n",
    "    #Scrape till the target page, next page till the target page\n",
    "    for page in range(target_page_per_day):\n",
    "        scrape_one_page()\n",
    "        if current_page < target_page_per_day:\n",
    "            click_next_page()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'current_page' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-d1b47948df68>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mif\u001b[0m \u001b[0mcurrent_page\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mmax_page\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m    \u001b[1;31m#点开‘更多’ Open'more'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mdriver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_element_by_xpath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"//a[contains(text(),'更多')]\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclick\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;31m#开始年月日 Start data e.g, 20200101\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'current_page' is not defined"
     ]
    }
   ],
   "source": [
    "if current_page == max_page:\n",
    "   #点开‘更多’ Open'more'\n",
    "    driver.find_element_by_xpath(\"//a[contains(text(),'更多')]\").click()\n",
    "    \n",
    "    #开始年月日 Start data e.g, 20200101\n",
    "    elem = driver.find_element_by_xpath(\"//*[@name='starttime']\")\n",
    "    elem.send_keys(days_normal)\n",
    "\n",
    "    #结束年月日 End data e.g, 20200131\n",
    "    elem = driver.find_element_by_xpath(\"//*[@name='endtime']\")\n",
    "    elem.clear()\n",
    "    elem.send_keys(days_normal)\n",
    "    time.sleep(1)\n",
    "\n",
    "    #搜索 Search\n",
    "    driver.find_element_by_xpath(\"//*[@type='submit']\").click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def date_input()\n",
    "def scrape_by_one_page()\n",
    "def next_page_until(target_page_per_day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始自动登陆，若出现验证码手动验证.Loading\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'max_page' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-110-e6cc93cbd26a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0mpassword\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"rw297421\"\u001b[0m \u001b[1;31m#密码password\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[0mdriver\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwebdriver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mChrome\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;31m#可以换成firefox.Can use firefox\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m     \u001b[0mscrape_by_date\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2010\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[1;31m#页数可以设置成max_page\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-110-e6cc93cbd26a>\u001b[0m in \u001b[0;36mscrape_by_date\u001b[1;34m(target_year, target_page_per_day)\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mdate\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdays_normal\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[0mscrape_one_page\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m         \u001b[0mnext_page_until\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget_page_per_day\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m         \u001b[0mdate_input_and_search_reiterate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'爬完'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mdate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-107-c75b70fd6d5a>\u001b[0m in \u001b[0;36mnext_page_until\u001b[1;34m(target_page_per_day)\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[0mtarget_page_per_day\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget_page_per_day\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0mtarget_page_per_day\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mmax_page\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'超出最大页数，改为爬最大页数'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mmax_page\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Over page limit. Instead, scrape to page limit'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mmax_page\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m: local variable 'max_page' referenced before assignment"
     ]
    }
   ],
   "source": [
    "def scrape_by_date(target_year,target_page_per_day):\n",
    "    login()\n",
    "    dates_in_year(target_year)\n",
    "    keyword_and_original()\n",
    "    for day in days_normal[:1]:\n",
    "        scrape_one_page()\n",
    "        next_page_until(target_page_per_day)\n",
    "        date_input_and_search(day)\n",
    "        print('Finish' + day)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    username = \"17064888409\" #用户名username\n",
    "    password = \"rw297421\" #密码password\n",
    "    driver = webdriver.Chrome()#可以换成firefox.Can use firefox        \n",
    "    scrape_by_date(2010, 20)\n",
    "#页数可以设置成max_page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
